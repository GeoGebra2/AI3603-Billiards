## 项目目标
- 在不修改 `poolenv.py` 的前提下，实现并接入一个可对战的 `NewAgent`，提升对战 `BasicAgent` 的胜率。
- 完成训练与评估全流程，使结果可复现。
- 在代码生成完成后，提供一份中文介绍文档，说明算法、使用方式与评估结果。

## 工作范围
- 仅修改/新增：`agent.py`（实现 `NewAgent`）、必要时 `utils.py`（评估期允许）、`evaluate.py`（接入 `agent_b`）。
- 不修改：`poolenv.py`（环境与规则）、`GAME_RULES.md`、`PROJECT_GUIDE.md`、`README.md`。

## 实现方案
- 核心接口：在 `agent.py` 中实现 `NewAgent.decision(*obs)`，输出五元动作参数 `V0, phi, theta, a, b`。
- 动作生成：
  - 候选击球生成：基于台面状态生成若干合法目标（优先己方球；清空后瞄准黑 8）。
  - 物理仿真评估：调用环境/仿真工具对每个候选动作进行快速模拟，记录进球、犯规、母球位置、下一杆可行性等。
  - 评分函数：综合进球收益、犯规风险、黑 8 终局可达性、解球与防守价值；对噪声鲁棒性进行加权（多次采样或不敏感度估计）。
  - 搜索与微调：对评分前几名动作做局部参数微调（如网格/贝叶斯优化/坐标下降），保留最高分动作作为输出。
- 策略结构：
  - 开局/中局：进球优先 + 解球（散球）策略；不可进球时优先安全球（让对手难以发挥）。
  - 终局：当己方球清空后，转向黑 8 的最佳路径，严控犯规项。
- 噪声处理：
  - 训练阶段可按 `agent.py` 174-182 行的 `noise_std` 配置增减 Agent 侧噪声（评估保持默认）。
  - 对候选动作做鲁棒性评分（如对参数添加高斯扰动取期望得分）。

## 训练与评估
- 训练：
  - 方案一（快速可复现）：启发式 + 贝叶斯优化微调评分权重与动作参数范围。
  - 方案二（可选加强）：RL（如 PPO/SAC）或 MCTS，训练脚本置于 `train/`，产出 `checkpoint`。
- 评估：
  - 在 `evaluate.py` 中将 `agent_b = NewAgent()` 接入，按项目规则进行 40 局对战。
  - 保存并报告 `AGENT_B_SCORE` 与胜率；输出对战统计。

## 代码改动点
- `agent.py`：
  - 新增 `class NewAgent`，实现 `__init__`（可加载 checkpoint）、`decision`（核心）。
  - 复用/封装通用工具（如几何计算、候选生成、评分函数）。
- `evaluate.py`：
  - 将 `agent_b` 指向 `NewAgent()`，其余公平性轮换保持不变。
- `utils.py`（如有需要）：
  - 放置通用几何/评分工具，保持评估允许的范围内改动。

## 验证与交付
- 单元/集成验证：
  - 对 `decision` 做若干静态台面用例（含可进球/不可进球）检查合法性与稳定性。
  - 运行 40 局评估，确认统计输出与胜率计算正确。
- 性能验证：
  - 对比 `BasicAgent` 胜率，目标 ≥ 50% 起步，迭代提升。

## 介绍文档（交付后提供）
- 文档结构：
  1. 项目概述与目标
  2. 算法设计（候选生成、仿真评估、评分函数、搜索微调、鲁棒性）
  3. 代码结构与改动说明（文件与关键类/函数）
  4. 使用指南（环境、运行命令、参数说明）
  5. 评估结果与分析（胜率、典型局面）
  6. 可扩展与改进方向（RL/MCTS 接入、权重学习、对抗训练）
- 形式：中文 Markdown，随代码一并交付。

## 运行与复现（参考）
- 安装依赖：`pip install -r requirements.txt`
- 评估命令：`python evaluate.py`（40 局），查看终端统计输出。

请确认以上计划；确认后我将开始实现 `NewAgent`、接入评估并生成介绍文档。